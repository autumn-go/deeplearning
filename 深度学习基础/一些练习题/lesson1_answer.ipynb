{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简答题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.列出深度学习中常见的激活函数及其导数\n",
    "> [参考](https://blog.csdn.net/yz930618/article/details/79165186)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sigmoid \n",
    "\n",
    "$$ f(z) = \\frac{1}{1+e^(-z)}$$\n",
    "$$f^\\prime(z) = f(z)[1-f(z)]$$\n",
    "\n",
    "* Tanh\n",
    "\n",
    "$$f(z) = tanh(z)$$\n",
    "$$f^\\prime(z) = 1 - [f(z)]^2$$\n",
    "\n",
    "* relu\n",
    "\n",
    "$$ f(z) = max(0,z) $$\n",
    "$$f^\\prime(z) = \\begin{cases} 0, z<0\\\\  1,z >0 \\\\  undefind, z = 0\\end{cases} $$\n",
    "\n",
    "* Leaky ReLU\n",
    "$$f\\left ( x \\right )=\\left\\{\\begin{matrix}\n",
    "  x   & x\\geqslant 0 \\\\ \n",
    "0.1*x & x< 0\n",
    "\\end{matrix}\\right.$$\n",
    "$$\\frac{\\partial f\\left ( x \\right )}{\\partial x}=\\left\\{\\begin{matrix}\n",
    " 1 & x\\geq 0 \\\\ \n",
    "0.1 & x< 0 \n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "* ELU\n",
    "$$f\\left ( x \\right )=\\left\\{\\begin{matrix}\n",
    "  x   & x\\geqslant 0 \\\\ \n",
    "\\alpha \\left ( e^{x}-1 \\right ) & x< 0\n",
    "\\end{matrix}\\right.$$\n",
    "$$\\frac{\\partial f\\left ( x \\right )}{\\partial x}=\\left\\{\\begin{matrix}\n",
    "x& x\\geq 0 \\\\ \n",
    "\\alpha e^{x}& x< 0 \n",
    "\\end{matrix}\\right.$$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.如何理解深度学习中的激活函数导致的梯度消失或梯度爆炸的问题？\n",
    "> [参考](https://blog.csdn.net/qq_25737169/article/details/78847691)  \n",
    "> [参考](https://blog.csdn.net/a8039974/article/details/77983854)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 梯度消失  \n",
    "> 当我们在做反向传播，计算损失函数对权重的梯度时，随着越向后传播，梯度变得越来越小，这就意味着在网络的前面一些层的神经元，会比后面的训练的要慢很多，甚至不会变化,直至消失\n",
    "* 梯度爆炸\n",
    "> 梯度爆炸就是由于初始化权值过大，前面层会比后面层变化的更快，就会导致权值越来越大，梯度爆炸的现象就发生了。  \n",
    "> 如果对每一层激活函数求导的值大于1，那么当层数增多时，梯度以指数级更新，即发生梯度爆炸。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Hinge Loss与Cross Entropy 这两种损失函数的不同点有哪些？\n",
    "> [Hinge Loss参考](https://blog.csdn.net/u010976453/article/details/78488279)  \n",
    "> [Cross Entropy交叉熵参考](https://blog.csdn.net/qq_25552539/article/details/78255217)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hinge Loss  \n",
    "\n",
    "二次代价函数的更新速度和激活函数的导数成正比\n",
    "* Cross Entropy  \n",
    "\n",
    "根据上个话题我们知道了激活函数为sigmoid、损失 函数为二次代价函数时，w、b的跟新速度与激活函数的导数相关。\n",
    "\n",
    "那么损失函数换成cross-entropy会怎样呢\n",
    "\n",
    "再来一波公式：\n",
    "\n",
    "![](https://img-blog.csdn.net/20171017201713810?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjU1NTI1Mzk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)\n",
    "\n",
    "由以上推导可得w、b对损失函数的偏导，即w、b的更新速度只与（a-y）有关，也就是只与预测值与真实值的差距有关。不同于二次代价函数的更新速度和激活函数的导数成正比。\n",
    "\n",
    "这样就会有一个比较好的性质：\n",
    "\n",
    "计算值与实际值差距越大，w、b的更新速度越快"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.简述神经网络BP算法中的前向运算，反向传播与参数更新\n",
    "> [参考](https://blog.csdn.net/cc514981717/article/details/73832119)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 前向运算  \n",
    "\n",
    "前向算法的作用是计算输入层结点对隐藏层结点的影响，也就是说，把网络正向的走一遍：输入层—->隐藏层—->输出层\n",
    "计算每个结点对其下一层结点的影响。 \n",
    "\n",
    "* 反向传播\n",
    "\n",
    "在实际情况中，因为是随机给定的权值，很大的可能（几乎是100%）得到的输出与实际结果之间的偏差非常的大，这个时候我们就需要比较我们的输出和实际结果之间的差异，将这个残差返回给整个网络，调整网络中的权重关系。这也是为什么我们在神经网络中需要后向传播的原因。  \n",
    "\n",
    "\n",
    "\n",
    "* 参数更新  \n",
    "\n",
    "计算总误差—->隐藏层的权值更新—->更新权重  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.梯度下降法BGD、SGD、Mini-batch SGD的区别？\n",
    "> [参考](http://www.cnblogs.com/wujingqiao/p/9559969.html)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BGD  \n",
    "\n",
    "我们平时说的梯度现将也叫做最速梯度下降，也叫做批量梯度下降(Batch Gradient Descent)。  \n",
    "\n",
    "在梯度下降中，对于参数的更新，需要计算所有的样本然后求平均，其计算得到的是一个标准梯度(这是一次迭代，我们其实需要做n次迭代直至其收敛)。因而理论上来说一次更新的幅度是比较大的。\n",
    "\n",
    "* SGD  \n",
    "\n",
    "与BGD相比，随机也就是说我每次随机采用样本中的一个例子来近似我所有的样本，用这一个随机采用的例子来计算梯度并用这个梯度来更新参数，即SGD每次迭代仅对一个随机样本计算梯度，直至收敛。  \n",
    "\n",
    "    * 由于SGD每次迭代只使用一个训练样本，因此这种方法也可用作online learning。\n",
    "\n",
    "    * 每次只使用一个样本迭代，若遇上噪声则容易陷入局部最优解。参数收敛过程变得较为抖动，损失函数的方差较大。  \n",
    "    \n",
    "* mini-batch SGD  \n",
    "\n",
    "他用了一些小样本来近似全部的，其本质就是既然SGD中1个样本的近似不一定准，那就用更大的30个或50(batch_size)个样本来近似，即mini-batch SGD每次迭代仅对n个随机样本计算，直至收敛。  \n",
    "MGD,小批量随机梯度下降，每次更新权重从所有样本随机选择一小部分进行计算，损失函数抖动比SGD小，计算复杂度比BGD低。 \n",
    "\n",
    "    * 随机在训练集中选取一个mini-batch，每个mini-batch包含n个样本；（n<N，N为总训练集样本数）\n",
    "\n",
    "    * 在每个mini-batch里计算每个样本的梯度，然后在这个mini-batch里求和取平均作为最终的梯度来更新参数；（注意虽然这里好像用到了BGD，但整体整体mini-batch的选择是用到了SGD）\n",
    "\n",
    "    * 以上两步可以看做是一次迭代，这样经过不断迭代，直至收敛"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
